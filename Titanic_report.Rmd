---
title: "Kaggle Titanic Data Analysis Report"
author: "P. Adames"
date: "4/2/2020"
output:
  html_document: default
  pdf_document: default
bibliography: titanic.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Transforming the Kaggle data into data frames

The Kaggle API for command line was used to get the data to start this analysis.

After installing the Kaggle API (Kaggle API 1.5.6), from the comamnd line, following  <https://www.kaggle.com/docs/api>:

```
$ kaggle competitions download -c titanic
    Downloading titanic.zip to /home/pablo/Documents/Winter 2020/ENSF 611/Project/titanic
    0%|                                                   | 0.00/34.1k [00:00<?, ?B/s]
    100%|
```

A new folder called `data/` was created under the project root directory and the file was moved there. The following R command inspects what's in the file without actually decompressing it.

```{r exploring zip file}
unzip("data/titanic.zip", list = TRUE)
```


Then the `.csv` files were extracted, stored in R compressed data format, `.rds`, for back up as invidivual sets of train and test sets, as well as a sample of how data must be submitted for scoring. Data frames were then populated with this data and supplied in memory for the exploratory phase.

```{r from zip file to data frames leaving a back up behind, echo=TRUE}
create_files <- function (fname,...) {
    try(expr = read.csv(unzip(zipfile = ..., files = c(fname))),silent = TRUE);
}

extract_file_names <- function (names) {
    setNames((unlist(strsplit(apply(names, MARGIN = c(1), function(r) r[1]), " "))), NULL)
}


create_df_from_zip_file <- function(file_name) {
    if(file.exists(file_name)) {
        files_from_kaggle <- unzip(file_name, list = TRUE)
        names <- extract_file_names(files_from_kaggle)
        dfs <- lapply(names, create_files, file_name)
        dfs
    }
}
```
The create a list of three data frames with the Kaggle data for the Titanic data analysis project

```{r creating the back up rds files}
dfs <- create_df_from_zip_file("data/titanic.zip")

if (dim(dfs[[1]])[2] == 2) { saveRDS(object = dfs[[3]], file = "data/sample_submission.rds")}
if (dim(dfs[[2]])[2] == 11) { saveRDS(object = dfs[[2]], file = "data/test.rds")}
if (dim(dfs[[3]])[2] == 12) { saveRDS(object = dfs[[3]], file = "data/train.rds")}

rm(dfs)
rm('create_files')
rm('extract_file_names')
rm('create_df_from_zip_file')
```

Now the data frames are generated from the backups.
```{r "train and test data frame creation from backups"}
titanic_train <- readRDS("data/train.rds")
titanic_test <- readRDS("data/test.rds")
out1<-paste0("Train data is ", dim(titanic_train)[1], " rows by ", dim(titanic_train)[2], " columns")
out2<-paste0("Test data is ", dim(titanic_test)[1], " rows by ", dim(titanic_test)[2], " columns")
print(out1)
print(out2)
```
This is the preallocated train/test split given by Kaggle.


## Data cleaning

Clean up all rows with mising values.

```{r remove missing values}
titanic_train_clean <- titanic_train[complete.cases(titanic_train), ]
dim(titanic_train_clean)
titanic_test_clean <- titanic_test[complete.cases(titanic_test), ]
dim(titanic_test_clean)
```

As a result of removing all records with any `NA` there was a reduction of 
`r dim(titanic_train)[1]-dim(titanic_train_clean)[1]` records in the train set.
Similarly, `r dim(titanic_test)[1]-dim(titanic_test_clean)[1]` records were removed from the test data set.

## Data exploration

Using the R package `rattle` [@WilliamsGraham2011DMwR], some basic statistics were observed.

The actual proportion of passengers that died according to [@Wikipedia.titanic.passengers] was `r format(round(1496/2208 * 100, 2), nsmall = 2)`, the training data has a casualty ratio of `r format(round(196/(196+133) * 100, 2), nsmall=2)`. 
The distribution by age and the fraction by gender can be seen int he two plots below.

```{r "Age distribution by survival", echo=FALSE}
library(ggplot2)
theme_set(theme_classic())
g <- ggplot(titanic_train_clean, aes(x=Survived, y=Age, fill= Sex)) +
    geom_bar(stat="identity", width = 0.75, position = "dodge") +
    labs(title = "Bar plot for age and survivor status",subtitle = "Discriminated by gender")
plot(g)
```


```{r echo=FALSE, out.width='60%'}
knitr::include_graphics('./img/Rattle-HistSurvivedByGender.png')
```

A clustering analysis with KMeans and two clusters, one with 302 passengers and a second one with 27. Between the two they explain 71.12% of the point variability of the data.
The cluster centers are:

       Age     SibSp     Parch      Fare
1 30.11947 0.5430464 0.3874172  23.81876
2 33.10815 0.7037037 1.0740741 174.72669

Their plot appear below:

```{r echo=FALSE, out.width='60%'}
knitr::include_graphics('./img/Rattle-KMeansclustering.png')
```

These clusters may indicate that there are data does naturally separate in two groups but the meaning of these clusters isn't very clear. Another analysis shows that the number of clusters stabilizes after approximately 5.

### Exploration with a generalized linear model

Using the `rattle` package a quick generalize linear model with the `probit` function showed how the statistically significant coefficients were those of the variables:

    1. `SibSp`: number of siblings and/or spouse
    2. `Age`
    3. `TFC_Pclass(1,2]`: first class passengers
    4. `TFC_Pclass(2,3] : second class passengers
    5. `TFC_R01_TNM_Sex(0,1]`: the gender of the passenger, 0 for female, 1 for male    
    

```
Summary of the Probit Regression model (built using glm):

Call:
glm(formula = TFC_Survived ~ ., family = binomial(link = "probit"), 
    data = crs$dataset[crs$train, c(crs$input, crs$target)])

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.2590  -0.7393  -0.3666   0.5816   2.5985  

Coefficients:
                        Estimate  Std. Error z value   Pr(>|z|)    
(Intercept)            5.5917216 146.9544573   0.038    0.96965    
Age                   -0.0199283   0.0067510  -2.952    0.00316 ** 
SibSp                 -0.2234184   0.1020782  -2.189    0.02862 *  
Parch                 -0.0370586   0.1216786  -0.305    0.76070    
Fare                  -0.0001906   0.0024013  -0.079    0.93672    
EmbarkedC             -3.3562126 146.9542888  -0.023    0.98178    
EmbarkedQ             -2.8990208 146.9549656  -0.020    0.98426    
EmbarkedS             -3.3481248 146.9542837  -0.023    0.98182    
TFC_Pclass(1,2]       -0.6508577   0.2950677  -2.206    0.02740 *  
TFC_Pclass(2,3]       -1.3804582   0.3034635  -4.549 0.00000539 ***
TFC_R01_TNM_Sex(0,1]  -1.6522252   0.1792327  -9.218    < 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

### Numerical variables

The input numerical variables are `Age`, `SibSp`, `Parch`, and `Fare`.
The variables `SibSp`, the number od siblings and spouses, can be treated as acontinuous value between 0 and the max observed in the data, 5.


The variable `Age` has a distribution that is centered around central values so a min max scaler could be appropriate to aid some of the predictors deal with different scales among variables.


```{r "Age distribution", echo=FALSE}
library(ggplot2)
theme_set(theme_classic())
g <- ggplot(titanic_train_clean, aes(Age)) +
    geom_density(aes(fill=Sex), alpha=0.8) +
    labs(title = "Density plot for Age",subtitle = "Discriminated by gender")
plot(g)
```

```{r "SibSp distribution", echo=FALSE}
hist(titanic_train_clean$SibSp)
```
It also shows high values for skewness 2.265915 and kurtosis 5.217676. This indicates that this variable needs to be scaled with a standard scaler.

`Parch`, the number of parents and children accompanying the passenger, can also be treated as a continuous independent variable.

```{r "Parch distribution", echo=FALSE}
hist(titanic_train_clean$Parch)
```
As the histogram shows and the values of skewness 1.836532 kurtosis 3.152783 indicate it should also be normalized with a standard scaler.

Finaly `Fare` shows a very skewed distribution towards the low values. This is confirmed by the statistics of the variable: a mean of 36.20, a median of 16.70, a standard deviation of 52.52, skeness 4.050455, and kurtosis 24.262948. Therefore a standard scaler is th recommended preprocessing.

```{r "Fare distribution", echo=FALSE}
hist(titanic_train_clean$Fare)
```



### Categorical variables 

The input categorical variables are `Sex` for gender Female (0) or Male (1), `Pclass` for the passenger category assigned by the shipping company, 1st, 2nd, or 3rd, en encoded as 1, 2 or 3, and treated as a continuous variable here that will be normalized with min max.

The dependent or predicted variable is `Survived` that takes 196 (59.6%) negative outcomes  represented as 0 and 133 (40.4%) positive results represented as 1. 


## Preprocessing

Only the numerical variables identified in the previous section were normalized. The `preObj` should be used to train and predict with the test data set for consistency. 

```{r "standarization"}
library(caret)
preObj <- preProcess(titanic_train_clean[,c("Age","SibSp","Parch","Fare")], method = c("center","scale", "BoxCox"))
```

### Preparing the data set for model training

```{r "preparing the training data set for model building"}
# sibsbstd <- predict(preObj, titanic_train_clean[,c(6,7,8,9)])$SibSp
titanic_train_transformed <- predict(preObj, newdata = titanic_train_clean[,c("Age","SibSp","Parch","Fare")])
titanic_train_scaled <- cbind(titanic_train_clean[,c("PassengerId","Pclass","Sex","Embarked", "Survived")], titanic_train_transformed)
titanic_train_scaled$Survived <- as.factor(titanic_train_scaled$Survived)
titanic_train_scaled$Pclass <- as.factor(titanic_train_scaled$Pclass)
```

The variables `Name` and `Cabin` were dropped from this data frame because they hold very little meaning for prediction.


### Preparing the data set for prediction

The test set has to be transformed in the same fashion as the training set, in this case using the same preprocessing object.


```{r "preparing the test set for prediction", echo=TRUE}
titanic_test_transformed <- predict(preObj, titanic_test_clean[,c("Age","SibSp","Parch","Fare")])
titanic_test_scaled <- cbind(titanic_test_clean[,c("PassengerId","Pclass","Sex","Embarked")], titanic_test_transformed)
titanic_test_scaled$Pclass <- as.factor(titanic_test_scaled$Pclass)
```


# Model building


## Logistic regression

```{r "logistic regression", message=FALSE, warning=FALSE}
set.seed(112233)
default_glm_mod = train(form = Survived ~ .,
                        data = titanic_train_scaled,
                        trControl = trainControl(method = "cv", number = 5),
                        method = "glm",
                        family = "binomial")

default_glm_mod
```


## xGBoost

Lets run a default XGBoost model.

```{r "default xGBoost model", echo=FALSE}
set.seed(112233)
library(parallel) 
no_cores <- detectCores() - 1
library(doParallel)
cl <- makePSOCKcluster(no_cores)
registerDoParallel(cl)

grid_default <- expand.grid(
  nrounds = 100,
  max_depth = 4,
  eta = 0.025,
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

train_control <- caret::trainControl(
  method = "none",
  verboseIter = FALSE, # no training log
  allowParallel = TRUE # FALSE for reproducible results 
)

xgb_base <- caret::train(
  form = Survived ~ ., 
  data = titanic_train_scaled,
  trControl = train_control,
  tuneGrid = grid_default,
  method = "xgbTree",
  verbose = TRUE
)
stopCluster(cl)
xgb_base$bestTune
```

### Obtaining the predictions

Once the model is fitted to the training data the test data is used to make the prediction and the result is saved to a `.csv` file in the folder for results to be submitted to Kaggle.

```{r "preparing xgboost for submission to Kaggle"}
survived_pred <- predict(xgb_base, newdata = titanic_test_scaled)
survived_pred <- ifelse(survived_pred == 1, "0", "1")
xgb_default_to_kaggle <- data.frame(PassengerId=titanic_test_scaled$PassengerId,
                                    Survived=survived_pred,
                                    stringsAsFactors = TRUE)
write.csv(xgb_default_to_kaggle, file = "results/xgb_default.csv", row.names=FALSE, append = FALSE)
```


## Random forests

```{r default random forest}
set.seed(112233)
library(parallel) 
# Calculate the number of cores
no_cores <- detectCores() - 1

library(doParallel)
cl <- makePSOCKcluster(no_cores)
registerDoParallel(cl) 

default_rf_mod <- train(form = Survived ~ ., 
                        data = titanic_train_scaled,
                        method = "rf",
                        prox = TRUE)
stopCluster(cl)
default_rf_mod
```                        

### Obtaining the predictions

Once the model is fitted to the training data the test data is used to make the prediction and the result is saved to a `.csv` file in the folder for results to be submitted to Kaggle.

```{r "preparing xgboost for submission to Kaggle"}
survived_pred_rf <- predict(default_rf_mod, newdata = titanic_test_scaled)
survived_pred_rf <- ifelse(survived_pred == 1, "0", "1")
rf_default_to_kaggle <- data.frame(PassengerId=titanic_test_scaled$PassengerId,
                                    Survived=survived_pred_rf,
                                    stringsAsFactors = TRUE)
write.csv(rf_default_to_kaggle, file = "results/rf_default.csv", row.names=FALSE, append = FALSE)
```




Random forest with cross validation

```{r default random forest loocv}
set.seed(112233)
library(parallel) 
# Calculate the number of cores
no_cores <- detectCores() - 1

library(doParallel)
cl <- makePSOCKcluster(no_cores)
registerDoParallel(cl) 

default_rfcv_mod <- train(form = Survived ~ ., 
                        data = titanic_train_scaled,
                        method = "rf",
                        trControl= trainControl(method = "LOOCV", p = 0.1, allowParallel = TRUE),
                        prox = TRUE)
default_rfcv_mod
stopCluster(cl)
```                        

The random forest grwon with leave one out cross validation for the splitting on variables while growing trees, produces an accuracy of `r default_rfcv_mod$results$Accuracy[-1]` on the training set.


## Support Vector Machine

The support vector machine with the Radial basis Function kernel has sigma and cost as the hyperparameters to tune.

```{r support vector machine svmRadial default}


```

# References
